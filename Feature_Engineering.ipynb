{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Questions**\n",
        "\n",
        "## **1. What is a parameter?**  \n",
        "\n",
        "- In **Machine Learning** and **Statistics**, a **parameter** is a value that the model **learns** or **uses** to make predictions. It controls how the model behaves.\n",
        "\n",
        "### **Two Common Contexts of \"Parameter\":**\n",
        "\n",
        "#### 1. **In Machine Learning Models**\n",
        "\n",
        "- A parameter is **learned from data** during training. These values determine the model’s predictions.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "- In **Linear Regression**:  \n",
        "  ( y = mx + b )  \n",
        "  - `m` (slope) and `b` (intercept) are parameters.\n",
        "\n",
        "- In **Neural Networks**:  \n",
        "  - Weights and biases in each layer are parameters.\n",
        "\n",
        "#### 2. **In Functions (Programming)**\n",
        "\n",
        "- In Python or programming in general, a parameter is a variable **defined in a function signature**.\n",
        "\n",
        "```python\n",
        "def greet(name):  # 'name' is a parameter\n",
        "    print(\"Hello\", name)\n",
        "\n",
        "greet(\"Arijit\")    # \"Arijit\" is an argument\n",
        "```\n",
        "\n",
        "- ### **Key Differences:**\n",
        "\n",
        "| Context          | Parameter Example              | Meaning                                 |\n",
        "|------------------|-------------------------------|-----------------------------------------|\n",
        "| Machine Learning | Weights, coefficients          | Learned during training                 |\n",
        "| Programming      | `def func(param):`             | Placeholder used in function definition |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. What is correlation? What does negative correlation mean?**  \n",
        "\n",
        "- **Correlation** is a statistical measure that describes the relationship between two variables.  \n",
        "\n",
        " - **Positive correlation**: Both variables increase together.  \n",
        "\n",
        " - **Negative correlation**: One variable increases while the other decreases.  \n",
        "\n",
        " - **No correlation**: No relationship between the variables.  \n",
        "\n",
        "- A **negative correlation** means that as one variable increases, the other decreases.  \n",
        "\n",
        "- Example: **The more time spent watching TV, the lower the exam scores.**  \n",
        "\n",
        "---\n",
        "\n",
        "## **3. Define Machine Learning. What are the main components in Machine Learning?**  \n",
        "\n",
        "- **Machine Learning** is a subset of AI where computers learn patterns from data without being explicitly programmed.  \n",
        "\n",
        "- ### **Main Components:**  \n",
        "\n",
        "1. **Dataset** – Collection of data used for training.  \n",
        "\n",
        "2. **Features** – Independent variables.  \n",
        "\n",
        "3. **Model** – Mathematical function mapping inputs to outputs.  \n",
        "\n",
        "4. **Loss function** – Measures model performance.  \n",
        "\n",
        "5. **Optimizer** – Adjusts model parameters.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4. How does loss value help in determining whether the model is good or not?**  \n",
        "\n",
        "- In Machine Learning and Deep Learning, the **loss value** is a **numerical measure** that tells us **how well or poorly a model is performing** during training or evaluation. It's one of the most critical metrics to monitor when building predictive models.\n",
        "\n",
        "- **Loss** is the **difference between the predicted output** and the **actual output**.\n",
        "\n",
        "- It is computed by a **loss function** (also called cost function).\n",
        "\n",
        "- Examples:\n",
        "\n",
        "  - `Mean Squared Error (MSE)` for regression\n",
        "\n",
        "  - `Cross-Entropy Loss` for classification\n",
        "\n",
        "> The lower the loss, the better the model is **at that point**.\n",
        "\n",
        "\n",
        "| Term        | Meaning |\n",
        "|-------------|--------|\n",
        "| **High Loss** | Model's predictions are **far** from the actual values – poor performance |\n",
        "| **Low Loss**  | Model's predictions are **close** to actual values – better performance |\n",
        "| **Zero Loss** | Model predicts perfectly (rare in real-world scenarios) |\n",
        "\n",
        "- ### **Loss helps in determining Model Quality:**\n",
        "\n",
        "1. **Guides Learning (During Training)**  \n",
        "   \n",
        "   - The optimizer uses the loss value to update weights.\n",
        "   \n",
        "   - If the loss decreases over epochs, the model is learning.\n",
        "\n",
        "2. **Overfitting or Underfitting Detection**  \n",
        "   \n",
        "   - **Training loss ↓ but validation loss ↑** → Overfitting\n",
        "   \n",
        "   - **Both training and validation loss are high** → Underfitting\n",
        "\n",
        "3. **Model Comparison**  \n",
        "   \n",
        "   - You can compare different models or algorithms using their final loss values on validation/test sets.\n",
        "\n",
        "- ### **Loss vs. Accuracy**\n",
        "\n",
        "| Metric      | Focuses On                                  | Good For                    |\n",
        "|-------------|---------------------------------------------|-----------------------------|\n",
        "| **Loss**    | Magnitude of prediction error               | Optimization during training |\n",
        "| **Accuracy**| Count of correct predictions                | Performance evaluation       |\n",
        "\n",
        "> ✅ You can have **high accuracy with high loss** (e.g., class imbalance) or **low accuracy with low loss**, depending on the task.\n",
        "\n",
        "- ### **Example in Python**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Binary classification (just 2 classes for simplicity)\n",
        "\n",
        "X, y = X[y < 2], y[y < 2]\n",
        "\n",
        "# Split data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate loss\n",
        "\n",
        "loss = log_loss(y_test, y_pred_proba)\n",
        "print(f\"Log Loss: {loss:.4f}\")\n",
        "```\n",
        "\n",
        "### **Key Takeaways:**\n",
        "\n",
        "- **Loss measures how bad your model is**: lower = better.\n",
        "- Used **during training** to improve performance.\n",
        "- Helps to **diagnose problems** like overfitting.\n",
        "- Different tasks require different loss functions.\n",
        "- **Track both loss and accuracy** for reliable evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What are continuous and categorical variables?**  \n",
        "\n",
        "- **Continuous variables**: Numerical values that can take any value (e.g., height, weight).  \n",
        "\n",
        "- **Categorical variables**: Represent categories (e.g., gender, colors).  \n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do we handle categorical variables in Machine Learning? What are the common techniques?**  \n",
        "\n",
        "- ### **Handling Categorical Variables in Machine Learning:**\n",
        "\n",
        " - Categorical variables are those that contain label values rather than numerical values, such as \"Red,\" \"Green,\" \"Blue,\" or \"Male/Female.\" Machine learning models require numerical input, so categorical variables must be transformed into a numerical format. Below are the common techniques used for handling categorical data.\n",
        "\n",
        "- ### **Common Techniques for Handling Categorical Variables:**\n",
        "\n",
        "#### **1. Label Encoding**\n",
        "\n",
        "- Assigns each category a unique integer value.\n",
        "\n",
        "- Suitable for **ordinal data** (where order matters).\n",
        "\n",
        "- **Risk:** Can introduce an unintended ordinal relationship for non-ordinal categories.\n",
        "\n",
        "- #### **Example in Python:**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Low', 'Medium', 'High', 'Low', 'High']\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)  # Output: [1, 2, 0, 1, 0]\n",
        "```\n",
        "✅ **Use when**: The categorical variable has an inherent order.\n",
        "\n",
        "### **2. One-Hot Encoding (OHE)**\n",
        "\n",
        "- Converts categorical values into separate binary columns (0s and 1s).\n",
        "\n",
        "- Suitable for **nominal data** (where order does not matter).\n",
        "\n",
        "- **Risk:** Can increase dataset dimensionality if there are many unique categories (high cardinality).\n",
        "\n",
        "#### **Example in Python:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue']})\n",
        "encoded_df = pd.get_dummies(df, columns=['Color'])\n",
        "print(encoded_df)\n",
        "```\n",
        "✅ **Use when**: The number of unique categories is small.\n",
        "\n",
        "### **3. Target Encoding (Mean Encoding)**\n",
        "\n",
        "- Replaces categories with the **mean of the target variable** for each category.\n",
        "\n",
        "- Suitable for **high cardinality categorical features**.\n",
        "\n",
        "- **Risk:** Can lead to **data leakage** if not used properly.\n",
        "\n",
        "#### **Example in Python:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'Category': ['A', 'B', 'A', 'C', 'B', 'C'],\n",
        "                   'Target': [1, 0, 1, 1, 0, 0]})\n",
        "target_mean = df.groupby('Category')['Target'].mean()\n",
        "df['Category_encoded'] = df['Category'].map(target_mean)\n",
        "print(df)\n",
        "```\n",
        "✅ **Use when**: The categorical variable has many unique values.\n",
        "\n",
        "### **4. Frequency Encoding**\n",
        "\n",
        "- Replaces categories with their frequency in the dataset.\n",
        "\n",
        "- **Advantage:** Does not increase dimensionality like One-Hot Encoding.\n",
        "\n",
        "#### **Example in Python:**\n",
        "\n",
        "```python\n",
        "df['Category_Frequency'] = df['Category'].map(df['Category'].value_counts())\n",
        "print(df)\n",
        "```\n",
        "✅ **Use when**: The frequency of occurrence is relevant to the model.\n",
        "\n",
        "### **5. Binary Encoding**\n",
        "\n",
        "- Converts categories into **binary digits** and stores them in separate columns.\n",
        "\n",
        "- **Advantage:** Reduces dimensionality compared to One-Hot Encoding.\n",
        "\n",
        "- #### **Example in Python using `category_encoders`:**\n",
        "\n",
        "```python\n",
        "import category_encoders as ce\n",
        "\n",
        "df = pd.DataFrame({'Category': ['A', 'B', 'C', 'A', 'B']})\n",
        "encoder = ce.BinaryEncoder(cols=['Category'])\n",
        "df_encoded = encoder.fit_transform(df)\n",
        "print(df_encoded)\n",
        "```\n",
        "✅ **Use when**: There are many unique categories and One-Hot Encoding would create too many columns.\n",
        "\n",
        "- ## **Choosing the Right Encoding Method**\n",
        "\n",
        "| **Scenario** | **Best Encoding Method** |\n",
        "|-------------|-------------------------|\n",
        "| **Few unique categories (Nominal data)** | One-Hot Encoding |\n",
        "| **Few unique categories (Ordinal data)** | Label Encoding |\n",
        "| **High Cardinality Categorical Data** | Target Encoding, Frequency Encoding |\n",
        "| **Reducing Dimensionality** | Binary Encoding |\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "- Proper handling of categorical variables is crucial for building effective machine learning models. The choice of encoding depends on:\n",
        "\n",
        "✔ The number of unique categories  \n",
        "\n",
        "✔ Whether the variable is **ordinal** or **nominal**  \n",
        "\n",
        "✔ The impact of **dimensionality** on model performance  \n",
        "\n",
        "---\n",
        "\n",
        "## **7. What do you mean by training and testing a dataset?**  \n",
        "  \n",
        "### **1️⃣ Definition:**  \n",
        "\n",
        "- In Machine Learning (ML), we split data into **training** and **testing** datasets to evaluate a model's performance:  \n",
        "\n",
        "1. **Training Set:** Used to train the model (learn patterns).  \n",
        "\n",
        "2. **Testing Set:** Used to test the model's accuracy on unseen data.  \n",
        "\n",
        "This helps ensure the model generalizes well to new data.\n",
        "\n",
        "\n",
        "### **2️⃣ Split Data:**\n",
        "\n",
        "- If we train and test on the same data, the model might **memorize** patterns instead of learning **generalized rules**. This leads to **overfitting**, where the model performs well on training data but poorly on new data.\n",
        "\n",
        "### **3️⃣ Data Splitting Process:**\n",
        "\n",
        "- We typically divide the dataset as follows:\n",
        "\n",
        "| **Dataset Type** | **Purpose** | **Typical Size** |\n",
        "|-----------------|------------|-----------------|\n",
        "| **Training Set** | Trains the model | 70-80% of the data |\n",
        "| **Testing Set** | Evaluates the model | 20-30% of the data |\n",
        "\n",
        "For better evaluation, we may also use a **validation set**:\n",
        "| **Dataset Type** | **Purpose** | **Typical Size** |\n",
        "|-----------------|------------|-----------------|\n",
        "| **Validation Set** | Fine-tuning hyperparameters | 10-15% of the data |\n",
        "\n",
        "### **4️⃣ Example: Splitting Data in Python**\n",
        "\n",
        "- We use `train_test_split()` from `sklearn.model_selection` to split data:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # Features\n",
        "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # Labels\n",
        "\n",
        "# Split data (80% training, 20% testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display sizes\n",
        "\n",
        "print(f\"Training data: {len(X_train)} samples\")\n",
        "print(f\"Testing data: {len(X_test)} samples\")\n",
        "```\n",
        "🔹 `test_size=0.2` → 20% of data for testing, 80% for training  \n",
        "🔹 `random_state=42` → Ensures reproducibility  \n",
        "\n",
        "\n",
        "### **5️⃣ Training vs. Testing a Model:**\n",
        "\n",
        "#### **Step 1: Train the Model**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize model\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train model using training data\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "✅ The model learns patterns from `X_train` and `y_train`.\n",
        "\n",
        "#### **Step 2: Test the Model**\n",
        "\n",
        "```python\n",
        "# Predict using test data\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "✅ **The model is evaluated** on `X_test`, comparing predictions (`y_pred`) with actual labels (`y_test`).\n",
        "\n",
        "\n",
        "- ### **6️⃣ Summary:**\n",
        "\n",
        "✅ **Training Set:** Used to train the model.  \n",
        "\n",
        "✅ **Testing Set:** Used to evaluate performance on unseen data.  \n",
        "\n",
        "✅ **Splitting the data** prevents **overfitting** and ensures **generalization**.  \n",
        "\n",
        "✅ **Use `train_test_split()` in Python** to efficiently separate data.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. What is sklearn.preprocessing?**\n",
        "\n",
        "- `sklearn.preprocessing` is a module in **scikit-learn** that provides:  \n",
        "\n",
        " - **Scaling techniques** (StandardScaler, MinMaxScaler)  \n",
        "\n",
        " - **Encoding techniques** (OneHotEncoder, LabelEncoder)  \n",
        "\n",
        "---\n",
        "\n",
        "## **9. What is a Test set?**\n",
        "   \n",
        "- A **test set** is a subset of the dataset used to evaluate the final performance of a trained machine learning model. It is separate from the training and validation sets to ensure an unbiased assessment of how well the model generalizes to unseen data.\n",
        "\n",
        "- ### **Test Set is used to:**  \n",
        "\n",
        "1. **Evaluate Model Performance**: Measures how well the trained model performs on new data.\n",
        "\n",
        "2. **Prevent Overfitting**: Ensures the model does not just memorize training data but generalizes well.\n",
        "\n",
        "3. **Compare Different Models**: Helps select the best-performing model among different approaches.\n",
        "\n",
        "4. **Estimate Real-World Performance**: Mimics how the model will behave on unseen, real-world data.\n",
        "\n",
        "\n",
        "- ### **Difference Between Training, Validation, and Test Sets:**\n",
        "\n",
        "| **Dataset**  | **Purpose** | **Data Exposure** |\n",
        "|-------------|------------|----------------|\n",
        "| **Training Set** | Used to train the model | Model learns from this data |\n",
        "| **Validation Set** | Fine-tunes hyperparameters | Model sees this but does not learn from it |\n",
        "| **Test Set** | Evaluates the final model | Model never sees this during training |\n",
        "\n",
        "- ### **Splitting Data into Training and Test Sets in Python**\n",
        "\n",
        "- We can use `train_test_split()` from Scikit-Learn to divide a dataset.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (features X and target y)\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
        "\n",
        "# Splitting the dataset (80% training, 20% testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set:\", X_train.shape, y_train.shape)\n",
        "print(\"Test Set:\", X_test.shape, y_test.shape)\n",
        "```\n",
        "\n",
        "- ### **Common Test Set Sizes:**\n",
        "\n",
        " - The **test set size** is usually between **10-30%** of the total data, depending on the dataset size:\n",
        "\n",
        " - **Large datasets (millions of samples)** → **10% test set**\n",
        "\n",
        " - **Moderate datasets (thousands of samples)** → **20-25% test set**\n",
        "\n",
        " - **Small datasets (hundreds of samples)** → **30% test set**  \n",
        "\n",
        "- ### **Best Practices for Using a Test Set:**\n",
        "\n",
        "✅ **Never use the test set during training** – It should only be used for final evaluation.  \n",
        "\n",
        "✅ **Keep test data separate** – Avoid data leakage by not exposing test data during feature selection.  \n",
        "\n",
        "✅ **Use stratified sampling for imbalanced data** – Ensure class distribution remains consistent.  \n",
        "\n",
        "✅ **Shuffle data before splitting** – Helps create a balanced split.  \n",
        "\n",
        "- ### **Conclusion**\n",
        "\n",
        " - A **test set** is critical for assessing the true performance of a machine learning model. Proper data splitting and careful use of the test set help build **robust and generalizable models** for real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?**  \n",
        "\n",
        "- Using `train_test_split` from `sklearn.model_selection`:  \n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "- **80% training, 20% testing**  \n",
        "\n",
        "- ### **Step-by-Step Approach for a Machine Learning problem:**  \n",
        "\n",
        "1. **Define the problem statement.**  \n",
        "\n",
        "2. **Collect and clean data.**  \n",
        "\n",
        "3. **Perform Exploratory Data Analysis (EDA).**  \n",
        "\n",
        "4. **Preprocess the data (scaling, encoding, etc.).**  \n",
        "\n",
        "5. **Train machine learning models.**  \n",
        "\n",
        "6. **Evaluate performance using metrics.**  \n",
        "\n",
        "7. **Optimize model performance.**  \n",
        "\n",
        "8. **Deploy the model.**  \n",
        "\n",
        "---\n",
        "\n",
        "## **11. Why do we have to perform EDA before fitting a model to the data?**  \n",
        "\n",
        "- EDA helps in:  \n",
        "\n",
        " - Identifying missing values.  \n",
        "\n",
        " - Understanding feature distributions.  \n",
        "\n",
        " - Detecting correlations and patterns.  \n",
        "\n",
        " - Spotting outliers.  \n",
        "\n",
        "---\n",
        "\n",
        "## **12. What is correlation?**\n",
        "\n",
        "- ### **Correlation in Machine Learning and Statistics**\n",
        "\n",
        " - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It helps us understand whether and how strongly two variables are related.\n",
        "\n",
        "For example:\n",
        "\n",
        "- **Positive correlation**: As one variable increases, the other also increases (e.g., height and weight).\n",
        "\n",
        "- **Negative correlation**: As one variable increases, the other decreases (e.g., exercise and body fat).\n",
        "\n",
        "- **No correlation**: No apparent relationship between the two variables (e.g., shoe size and intelligence).\n",
        "\n",
        "- ### **Types of Correlation:-**\n",
        "\n",
        "### **1. Pearson’s Correlation Coefficient (Linear Correlation):**\n",
        "\n",
        "- Measures **linear** relationship between two variables.\n",
        "\n",
        "- Values range from **-1 to 1**:\n",
        "  - **+1** → Perfect positive correlation  \n",
        "  - **0** → No correlation  \n",
        "  - **-1** → Perfect negative correlation  \n",
        "\n",
        "- #### **Python Implementation:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Creating sample data\n",
        "\n",
        "data = {'X': [10, 20, 30, 40, 50], 'Y': [12, 24, 33, 45, 51]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate Pearson correlation\n",
        "\n",
        "correlation = df.corr(method='pearson')\n",
        "print(correlation)\n",
        "```\n",
        "**When to use**: The relationship is linear and both variables are continuous.\n",
        "\n",
        "### **2. Spearman’s Rank Correlation**\n",
        "\n",
        "- Measures **monotonic** relationships (not necessarily linear).\n",
        "\n",
        "- Based on ranking rather than actual values.\n",
        "\n",
        "- Useful when data has **outliers**.\n",
        "\n",
        "- #### **Python Implementation:**\n",
        "\n",
        "```python\n",
        "correlation_spearman = df.corr(method='spearman')\n",
        "print(correlation_spearman)\n",
        "```\n",
        "✅ **Use when**: The relationship is not strictly linear or the data contains outliers.\n",
        "\n",
        "### **3. Kendall’s Tau Correlation**\n",
        "\n",
        "- Measures the strength of association between two variables based on the **order** of data.\n",
        "\n",
        "- Less sensitive to small sample sizes.\n",
        "\n",
        "- #### **Python Implementation:**\n",
        "\n",
        "```python\n",
        "correlation_kendall = df.corr(method='kendall')\n",
        "print(correlation_kendall)\n",
        "```\n",
        "✅ **Use when**: The dataset is small or when measuring ordinal relationships.\n",
        "\n",
        "- ### **Correlation vs. Causation**\n",
        "\n",
        "🔹 **Correlation does NOT imply causation**  \n",
        "Just because two variables are correlated doesn’t mean one causes the other!  \n",
        "\n",
        "- Example: Ice cream sales and drowning rates have a strong positive correlation. But eating ice cream doesn’t cause drowning—it’s just that both increase in summer.\n",
        "\n",
        "- ### **Visualizing Correlation**\n",
        "\n",
        " - A **heatmap** is commonly used to visualize correlation between multiple variables.\n",
        "\n",
        "- #### **Python Code for Heatmap:**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a heatmap\n",
        "\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "- ### **Conclusion:**\n",
        "\n",
        "✔ **Correlation helps** in feature selection, reducing multicollinearity, and understanding variable relationships.\n",
        "\n",
        "✔ **Choose the right correlation metric** based on the type of data and relationship.  \n",
        "\n",
        "✔ **Be careful** not to confuse correlation with causation!\n",
        "\n",
        "---\n",
        "\n",
        "## **13. What does negative correlation mean?**\n",
        "\n",
        "### **Negative Correlation in Statistics:**  \n",
        "\n",
        "- A **negative correlation** (or **inverse correlation**) occurs when two variables move in opposite directions. This means:  \n",
        "\n",
        " - As **one variable increases**, the **other decreases**.  \n",
        "\n",
        " - As **one variable decreases**, the **other increases**.  \n",
        "\n",
        "- The strength of a negative correlation is measured by the **correlation coefficient (r)**, which ranges from **-1 to 1**:  \n",
        "\n",
        " - ( r = -1 ) → Perfect negative correlation (strongest inverse relationship).  \n",
        "\n",
        " - ( r = 0 ) → No correlation.  \n",
        "\n",
        " - ( r ) is closer to -1 → Strong negative correlation.  \n",
        "\n",
        "- ### **Example of Negative Correlation:**  \n",
        "\n",
        "1. **Stock Market & Gold Prices**: When stock prices fall, investors tend to buy gold, increasing gold prices.  \n",
        "\n",
        "2. **Speed & Travel Time**: The faster you drive, the less time it takes to reach a destination.  \n",
        "\n",
        "3. **Exercise & Body Fat**: The more you exercise, the lower your body fat percentage tends to be.  \n",
        "\n",
        "---\n",
        "\n",
        "## **14. How can you find correlation between variables in Python?**\n",
        "\n",
        "- Correlation is a statistical measure that describes the **relationship** between two variables. It helps determine how strongly one variable is related to another.  \n",
        "\n",
        "- ### **Types of Correlation:**  \n",
        "\n",
        "1. **Positive Correlation** → Both variables increase or decrease together.  \n",
        "\n",
        "2. **Negative Correlation** → One variable increases while the other decreases.  \n",
        "\n",
        "3. **Zero Correlation** → No relationship between the variables.  \n",
        "\n",
        "- ### **Methods to Find Correlation in Python:**  \n",
        "\n",
        "#### **1. Using Pandas `.corr()` Method:**\n",
        "\n",
        "Pandas provides a simple `.corr()` function to calculate correlation coefficients.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "\n",
        "data = {\n",
        "    'Age': [25, 30, 35, 40, 45],\n",
        "    'Salary': [40000, 50000, 60000, 70000, 80000],\n",
        "    'Experience': [1, 3, 5, 7, 10]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n",
        "```\n",
        "\n",
        "**By default, it uses Pearson correlation** (explained below).  \n",
        "\n",
        "#### **2. Different Correlation Methods in Pandas:**\n",
        "\n",
        "```python\n",
        "df.corr(method='pearson')   # Pearson correlation (default)\n",
        "df.corr(method='spearman')  # Spearman correlation\n",
        "df.corr(method='kendall')   # Kendall correlation\n",
        "```\n",
        "\n",
        "**Methods:**  \n",
        "\n",
        "- **Pearson** → Linear relationships  \n",
        "\n",
        "- **Spearman** → Monotonic relationships (rank-based)  \n",
        "\n",
        "- **Kendall** → Measures ordinal associations  \n",
        "\n",
        "#### **3. Visualizing Correlation Using a Heatmap (Seaborn):**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a heatmap\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "🔹 **Heatmap use:**  \n",
        "\n",
        "- It provides a **clear visualization** of how strongly variables are correlated.  \n",
        "\n",
        "#### **4. Finding Correlation Between Two Specific Variables:**\n",
        "\n",
        "```python\n",
        "correlation_value = df['Age'].corr(df['Salary'])\n",
        "print(f\"Correlation between Age and Salary: {correlation_value}\")\n",
        "```\n",
        "\n",
        "- ### **Interpreting Correlation Values:**\n",
        "\n",
        "| **Correlation Coefficient (r)** | **Interpretation** |\n",
        "|------------------|----------------|\n",
        "| **+1** | Perfect positive correlation |\n",
        "| **+0.7 to +0.9** | Strong positive correlation |\n",
        "| **+0.3 to +0.7** | Moderate positive correlation |\n",
        "| **0 to +0.3** | Weak positive correlation |\n",
        "| **0** | No correlation |\n",
        "| **-0.3 to 0** | Weak negative correlation |\n",
        "| **-0.7 to -0.3** | Moderate negative correlation |\n",
        "| **-1** | Perfect negative correlation |\n",
        "\n",
        "- ### **Conclusion:**\n",
        "\n",
        " - Use `.corr()` for quick correlation analysis.\n",
        "\n",
        " - Use heatmaps for better visualization.  \n",
        "\n",
        " - Choose **Pearson/Spearman/Kendall** based on data characteristics.  \n",
        "\n",
        " - Interpret values carefully to understand relationships.  \n",
        "\n",
        "- **Correlation helps in feature selection, understanding relationships, and improving machine learning models!**\n",
        "\n",
        "---\n",
        "\n",
        "## **15. What is causation? Explain the difference between correlation and causation with an example.**  \n",
        "\n",
        "- **Causation (or Causal Relationship)** means that one event **directly influences** another event. If variable A causes variable B, then changes in A will lead to changes in B.  \n",
        "\n",
        "- ### **Difference Between Correlation and Causation:**  \n",
        "\n",
        "| **Aspect**         | **Correlation**                                  | **Causation**                                  |\n",
        "|------------------|--------------------------------------------------|--------------------------------------------------|\n",
        "| **Definition**   | Measures the relationship between two variables. | Shows that one variable directly affects another. |\n",
        "| **Direction**    | No implied direction of influence.               | Implies a cause-effect relationship. |\n",
        "| **Third Variables** | May be influenced by a third factor (confounding variable). | A direct effect exists without external factors. |\n",
        "| **Example**      | Ice cream sales and drowning incidents increase in summer. | Eating unhealthy food leads to weight gain. |\n",
        "\n",
        "### **Example: Correlation vs Causation:-**  \n",
        "\n",
        "### **Example 1: Ice Cream Sales & Drowning:**  \n",
        "\n",
        "- **Observation:** Data shows that ice cream sales and drowning deaths increase together.  \n",
        "\n",
        "- **Correlation:** There is a positive correlation between ice cream sales and drowning.  \n",
        "\n",
        "- **Causation?:** NO! Ice cream does **not** cause drowning. The actual reason is **summer** — more people go swimming and also buy ice cream.  \n",
        "\n",
        "### **Example 2: Smoking & Lung Cancer:**  \n",
        "\n",
        "- **Observation:** Studies show that people who smoke regularly have a higher chance of lung cancer.  \n",
        "\n",
        "- **Correlation:** A strong positive correlation exists between smoking and lung cancer.  \n",
        "\n",
        "- **Causation?:** YES! Smoking contains harmful chemicals that directly **cause** lung cancer.  \n",
        "\n",
        "- ### **Key Takeaways:**\n",
        "\n",
        "✅ **Correlation does not imply causation.**  \n",
        "\n",
        "✅ **Causation requires experimental proof or logical reasoning.**  \n",
        "\n",
        "✅ **Be cautious with data interpretation — always check for third variables.**  \n",
        "\n",
        "- **Real-World Application:** In machine learning and data science, we analyze correlation but must avoid assuming causation without proper evidence!\n",
        "\n",
        "---\n",
        "\n",
        "## **16. What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "- An **Optimizer** adjusts model parameters to minimize loss.  \n",
        "\n",
        "### **Types of Optimizers:**  \n",
        "\n",
        "- **Gradient Descent** – Iterative optimization algorithm.  \n",
        "\n",
        "- **Adam Optimizer** – Adaptive moment estimation.  \n",
        "\n",
        "- **RMSprop** – Good for recurrent networks.  \n",
        "\n",
        "Example in TensorFlow:  \n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **17. What is sklearn.linear_model?**  \n",
        "\n",
        "- `sklearn.linear_model` is a module in **scikit-learn** that provides linear models like:  \n",
        "\n",
        " - **Linear Regression**  \n",
        "\n",
        " - **Logistic Regression**  \n",
        "\n",
        "- Example:  \n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **18. What does model.fit() do? What arguments must be given?**  \n",
        "\n",
        "### **1️⃣ Definition**\n",
        "\n",
        "- `model.fit()` is a method used in **machine learning models** to train the model on a given dataset. It helps the model learn patterns from the **input features (X)** and their corresponding **labels (Y)**.\n",
        "\n",
        "\n",
        "### **2️⃣ Working:**\n",
        "\n",
        "1. **Takes input features (X) and labels (Y)**\n",
        "\n",
        "2. **Optimizes model parameters** using an algorithm (e.g., gradient descent)\n",
        "\n",
        "3. **Finds the best-fit function** to map\n",
        "\n",
        "4. **Stores the learned parameters** for future predictions\n",
        "\n",
        "### **3️⃣ Arguments for `model.fit()`:**\n",
        "\n",
        "- The function requires **at least two arguments**:\n",
        "\n",
        "| **Argument** | **Description** |\n",
        "|-------------|----------------|\n",
        "| `X` (array-like, DataFrame) | Input features (independent variables) |\n",
        "| `Y` (array-like, Series) | Target labels (dependent variable) |\n",
        "\n",
        "#### **Optional Arguments (for some models):**\n",
        "\n",
        "| **Argument** | **Description** |\n",
        "|-------------|----------------|\n",
        "| `epochs` | Number of times the model sees the dataset (for neural networks) |\n",
        "| `batch_size` | Number of samples per gradient update |\n",
        "| `verbose` | Controls logging of training progress |\n",
        "| `validation_data` | Data used for validation |\n",
        "\n",
        "\n",
        "### **4️⃣ Example 1: Using `model.fit()` for Regression**\n",
        "\n",
        "- **Linear Regression Training:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Training data\n",
        "\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "# Initialize and train the model\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model is now trained and ready for predictions!\n",
        "```\n",
        "🔹 **Interpretation:** The model learns the relationship.\n",
        "\n",
        "\n",
        "### **5️⃣ Example 2: Using `model.fit()` for Classification:**\n",
        "\n",
        "- **Logistic Regression Training**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Training data\n",
        "\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([0, 0, 1, 1, 1])  # Binary classification labels\n",
        "\n",
        "# Initialize and train model\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Now, the model is trained and can predict new data!\n",
        "```\n",
        "🔹 **Interpretation:** The model learns to classify **0 or 1** based on input.\n",
        "\n",
        "\n",
        "### **6️⃣ Example 3: Training a Neural Network (Deep Learning):**\n",
        "\n",
        "- **Using Keras with `fit()`**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Generate dummy training data\n",
        "\n",
        "X_train = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
        "y_train = np.random.randint(0, 2, size=(1000,))  # Binary labels (0 or 1)\n",
        "\n",
        "# Define a simple neural network\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with `fit()`\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "```\n",
        "🔹 **Interpretation:**  \n",
        "\n",
        "- `epochs=10` → Model sees the dataset **10 times**\n",
        "\n",
        "- `batch_size=32` → Trains on **32 samples per step**\n",
        "\n",
        "- `verbose=1` → Shows training progress\n",
        "\n",
        "\n",
        "- ### **7️⃣ When to Use `fit()`:**\n",
        "\n",
        "| **Scenario** | **Use `fit()`?** |\n",
        "|-------------|----------------|\n",
        "| Training a **supervised ML model** | ✅ Yes |\n",
        "| Training a **deep learning model** | ✅ Yes |\n",
        "| Making predictions | ❌ Use `predict()` instead |\n",
        "| Loading a pre-trained model | ❌ Use `load_model()` instead |\n",
        "\n",
        "\n",
        "- ### **8️⃣ Summary:**\n",
        "\n",
        "✅ `model.fit(X, y)` trains the model using given input-output pairs.  \n",
        "\n",
        "✅ Used in **both classification & regression models**.  \n",
        "\n",
        "✅ Can handle **batch training, epochs, and validation**.  \n",
        "\n",
        "✅ **After training, use `predict()`** for new data.\n",
        "\n",
        "---\n",
        "\n",
        "## **19. What does model.predict() do? What arguments must be given?**  \n",
        "\n",
        "### **1️⃣ Definition:**  \n",
        "\n",
        "- `model.predict()` is a method used in **machine learning models** to generate predictions on **new (unseen) data** after training.\n",
        "\n",
        "### **2️⃣ Working:**\n",
        "\n",
        "1. **Takes input features (X)**\n",
        "\n",
        "2. **Uses the trained model** to compute predictions  \n",
        "\n",
        "3. **Returns predicted values**\n",
        "\n",
        "   - For regression: Continuous values  \n",
        "   \n",
        "   - For classification: Class probabilities or labels  \n",
        "\n",
        "\n",
        "### **3️⃣ Arguments for `model.predict()`:**\n",
        "\n",
        "- The function takes **one required argument**:\n",
        "\n",
        "| **Argument**  | **Description**  |\n",
        "|--------------|----------------|\n",
        "| **X (array-like or DataFrame)** | The input feature(s) for prediction |\n",
        "\n",
        "#### **Optional Arguments (for some models):**\n",
        "| **Argument**  | **Description**  |\n",
        "|--------------|----------------|\n",
        "| `batch_size` | Number of samples per batch (for large datasets) |\n",
        "| `verbose` | Prints logs (useful for deep learning models) |\n",
        "\n",
        "\n",
        "### **4️⃣ Example 1: Using `model.predict()` for Regression:-**\n",
        "\n",
        "- **Linear Regression Prediction:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Training data\n",
        "\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "# Train model\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict new values\n",
        "\n",
        "X_test = np.array([[6], [7]])\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(predictions)  # Output: [12. 14.]\n",
        "```\n",
        "🔹 **Interpretation:** If \\(X = 6\\), the model predicts \\(Y = 12\\).\n",
        "\n",
        "\n",
        "- ### **5️⃣ Example 2: Using `model.predict()` for Classification:-**\n",
        "\n",
        "- **Logistic Regression Prediction:**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Training data\n",
        "\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([0, 0, 1, 1, 1])  # Binary labels\n",
        "\n",
        "# Train model\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels\n",
        "\n",
        "X_test = np.array([[2.5], [4.5]])\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(predictions)  # Output: [0 1] (Predicted classes)\n",
        "```\n",
        "🔹 **Interpretation:** If \\(X = 2.5\\), the model predicts **Class 0**.\n",
        "\n",
        "- **Predicting Probabilities**\n",
        "\n",
        "```python\n",
        "prob_predictions = model.predict_proba(X_test)\n",
        "print(prob_predictions)  # Probabilities for each class\n",
        "```\n",
        "🔹 **Interpretation:** Returns probabilities instead of class labels.\n",
        "\n",
        "\n",
        "| **Scenario** | **Use `predict()`?** |\n",
        "|-------------|----------------|\n",
        "| Predicting **continuous values** (e.g., price, temperature) | ✅ Yes (Regression) |\n",
        "| Predicting **class labels** (e.g., spam/not spam) | ✅ Yes (Classification) |\n",
        "| Predicting **probabilities** (e.g., probability of default) | ❌ Use `predict_proba()` instead |\n",
        "\n",
        "\n",
        "- ### **7️⃣ Summary**\n",
        "\n",
        "✅ `model.predict(X)` generates predictions for new inputs.  \n",
        "\n",
        "✅ Used in **both classification & regression models**.  \n",
        "\n",
        "✅ **For probabilities**, use `predict_proba()`.  \n",
        "\n",
        "✅ **Ensure correct input format** (array-like, DataFrame, or NumPy array).\n",
        "\n",
        "---\n",
        "\n",
        "## **20. What are continuous and categorical variables?**\n",
        "\n",
        "- ### **Continuous and Categorical Variables in Machine Learning:**  \n",
        "\n",
        " - In statistics and machine learning, variables are categorized into **continuous** and **categorical** types based on their nature and the type of values they hold.  \n",
        "\n",
        "### **1. Continuous Variables**  \n",
        "\n",
        "- A **continuous variable** is a numeric variable that can take **any value within a range** (including decimals and fractions). These variables are measured rather than counted.  \n",
        "\n",
        "- ### **Characteristics:**  \n",
        "\n",
        "✅ Can have an infinite number of values within a given range.  \n",
        "\n",
        "✅ Represent measurable quantities.  \n",
        "\n",
        "✅ Arithmetic operations (addition, subtraction, multiplication) make sense.  \n",
        "\n",
        "- ### **Examples:**  \n",
        "\n",
        "- **Height** (e.g., 5.8 feet, 180.5 cm)  \n",
        "\n",
        "- **Weight** (e.g., 65.4 kg, 150.2 lbs)  \n",
        "\n",
        "- **Temperature** (e.g., 22.5°C, 98.6°F)  \n",
        "\n",
        "- **Stock Prices** (e.g., ₹150.75, ₹420.10)  \n",
        "\n",
        "- ### **2. Categorical Variables**  \n",
        "\n",
        " - A **categorical variable** (also called a **qualitative** or **discrete** variable) represents groups or categories and **cannot be measured numerically** in a meaningful way.  \n",
        "\n",
        "- ### **Characteristics:**  \n",
        "\n",
        "✅ Represents labels or groups.  \n",
        "\n",
        "✅ Can be **nominal** (no order) or **ordinal** (ordered categories).  \n",
        "\n",
        "✅ Arithmetic operations do **not** make sense.  \n",
        "\n",
        "- ### **Examples:**  \n",
        "\n",
        " - **Gender** (Male, Female, Non-binary)  \n",
        "\n",
        " - **Blood Type** (A, B, AB, O)  \n",
        "\n",
        " - **Marital Status** (Single, Married, Divorced)  \n",
        "\n",
        " - **Education Level** (High School, Bachelor's, Master's, PhD)  \n",
        "\n",
        "- ### **Key Differences Between Continuous and Categorical Variables:**  \n",
        "\n",
        "| Feature             | Continuous Variable | Categorical Variable |\n",
        "|---------------------|--------------------|----------------------|\n",
        "| **Type of Values**  | Measurable numbers (decimals, fractions) | Groups or categories |\n",
        "| **Possible Values** | Infinite (within a range) | Limited set of values |\n",
        "| **Examples**        | Age, Weight, Income | Gender, City, Product Category |\n",
        "| **Arithmetic Operations** | Meaningful | Not meaningful |\n",
        "| **Subtypes**        | Interval, Ratio | Nominal, Ordinal |\n",
        "\n",
        "- ### **Python Example to Identify Variable Types:**  \n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Age': [25, 30, 35, 40],         # Continuous\n",
        "    'Income': [50000, 60000, 75000, 90000],  # Continuous\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male'],  # Categorical\n",
        "    'Education': ['Bachelor', 'Master', 'PhD', 'Bachelor']  # Categorical\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Checking variable types\n",
        "print(df.dtypes)\n",
        "```\n",
        "\n",
        "- ### **Output:**  \n",
        "```\n",
        "Age          int64  (Continuous)\n",
        "Income       int64  (Continuous)\n",
        "Gender       object (Categorical)\n",
        "Education    object (Categorical)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **21. What is feature scaling? How does it help in Machine Learning?**  \n",
        "\n",
        "- Feature scaling **standardizes the range of data**, improving model performance.  \n",
        "\n",
        "- ### **Common Methods:**  \n",
        "\n",
        "1. **Standardization (Z-score scaling):**  \n",
        "   [\n",
        "   X' = {X - mu}{sigma}\n",
        "   ]\n",
        "\n",
        "2. **Normalization (Min-Max Scaling):**  \n",
        "   [\n",
        "   X' = {X - min(X)}{max(X) - min(X)}\n",
        "   ]\n",
        "\n",
        "---\n",
        "\n",
        "## **22. How do we perform scaling in Python?**  \n",
        "\n",
        "### **1️⃣ Feature Scaling:**  \n",
        "\n",
        "- Feature scaling is a technique used to **normalize or standardize** numerical features in a dataset. Machine learning algorithms perform better when features are on a similar scale, preventing bias toward larger values.\n",
        "\n",
        "\n",
        "### **2️⃣ Importance of Feature Scaling:**  \n",
        "\n",
        "- Prevents dominance of large numerical values (e.g., `income in thousands` vs. `age in years`).  \n",
        "\n",
        "- Essential for gradient-based algorithms like **Logistic Regression, SVM, Neural Networks**.  \n",
        "\n",
        "- Improves **convergence speed** of optimization algorithms.  \n",
        "\n",
        "- Required for distance-based models like **KNN, K-Means, PCA**.\n",
        "\n",
        "\n",
        "### **3️⃣ Common Feature Scaling Techniques:-**  \n",
        "\n",
        "- ### **(i) Min-Max Scaling (Normalization):**\n",
        "\n",
        "- Scales features to a range of **[0, 1]**.  \n",
        "\n",
        "- **Best for:** Algorithms that assume **bounded values** (e.g., Neural Networks).  \n",
        "\n",
        "- **Python Implementation:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[50], [30], [90], [100], [70]])  # Example data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)  # Output: Values scaled between 0 and 1\n",
        "```\n",
        "\n",
        "- ### **(ii) Standardization (Z-score Normalization)**\n",
        "\n",
        "- Scales features to have **zero mean** and **unit variance**.   \n",
        "\n",
        "- **Python Implementation:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)  # Output: Mean = 0, Std Dev = 1\n",
        "```\n",
        "\n",
        "- ### **(iii) Robust Scaling (Handles Outliers)**\n",
        "\n",
        "- Uses **median** and **interquartile range (IQR)** instead of mean and standard deviation.  \n",
        "\n",
        "- **Python Implementation:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "```\n",
        "\n",
        "### **(iv) MaxAbs Scaling:**\n",
        "\n",
        "- Scales data by dividing by the **maximum absolute value** of the feature.  \n",
        "- **Best for:** Data that is **already centered** at 0.  \n",
        "\n",
        "👉 **Python Implementation:**  \n",
        "```python\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "```\n",
        "\n",
        "- ### **4️⃣ Choosing the Right Scaling Method:**  \n",
        "\n",
        "| **Scaling Method**  | **When to Use?** |\n",
        "|------------------|----------------|\n",
        "| **Min-Max Scaling** | When data is bounded (e.g., image pixel values) |\n",
        "| **Standardization** | When data follows a normal distribution |\n",
        "| **Robust Scaling** | When data has outliers |\n",
        "| **MaxAbs Scaling** | When data is centered around 0 |\n",
        "\n",
        "\n",
        "- ### **5️⃣ Scaling Multiple Features at once:**\n",
        "\n",
        " - **Example with Pandas DataFrame:**  \n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.DataFrame({'Age': [25, 30, 35, 40], 'Salary': [50000, 60000, 70000, 80000]})\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_scaled)  # All features scaled\n",
        "```\n",
        "\n",
        "- ### **6️⃣ Conclusion**\n",
        "\n",
        "✅ Feature scaling is **crucial** for machine learning models.  \n",
        "\n",
        "✅ Choose the **appropriate scaling method** based on **data distribution** and **algorithm requirements**.  \n",
        "\n",
        "✅ Always apply **scaling after train-test split** to prevent **data leakage**.\n",
        "\n",
        "---\n",
        "\n",
        "## 23. **What is sklearn.preprocessing?**\n",
        "\n",
        "- `sklearn.preprocessing` is a module in **Scikit-learn** that provides functions and classes for **data preprocessing** before feeding it into machine learning models. This module is essential for **scaling, normalizing, encoding, and transforming** data to improve model performance.  \n",
        "\n",
        "- ### **Need of Preprocessing:**  \n",
        "\n",
        " - Raw data often contains:  \n",
        "\n",
        "✅ Different **scales** (e.g., income in thousands vs. age in years).  \n",
        "\n",
        "✅ **Categorical variables** that need encoding.  \n",
        "\n",
        "✅ **Missing values** or **outliers** that must be handled.  \n",
        "\n",
        "✅ Uneven distributions that require **transformation**.  \n",
        "\n",
        "- Preprocessing helps make data **more suitable for machine learning models** by improving accuracy and efficiency.  \n",
        "\n",
        "\n",
        "- ### **Key Functions in `sklearn.preprocessing`:**\n",
        "\n",
        " - Here are the most commonly used preprocessing techniques:  \n",
        "\n",
        "### **1. Feature Scaling (Normalization & Standardization)**  \n",
        "\n",
        "🔹 **StandardScaler** – Standardizes features to have **zero mean** and **unit variance**.  \n",
        "\n",
        "🔹 **MinMaxScaler** – Scales features to a fixed range (e.g., 0 to 1).  \n",
        "\n",
        "🔹 **RobustScaler** – Useful for **handling outliers**, scales using the median and IQR.  \n",
        "\n",
        "- **Example:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data\n",
        "\n",
        "data = np.array([[50], [20], [30], [100]])\n",
        "\n",
        "# Standardization (mean = 0, variance = 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(data)\n",
        "\n",
        "# Normalization (scales between 0 and 1)\n",
        "\n",
        "minmax_scaler = MinMaxScaler()\n",
        "normalized_data = minmax_scaler.fit_transform(data)\n",
        "\n",
        "print(\"Standardized Data:\\n\", standardized_data)\n",
        "print(\"Normalized Data:\\n\", normalized_data)\n",
        "```\n",
        "\n",
        "### **2. Encoding Categorical Variables**  \n",
        "\n",
        "🔹 **Label Encoding** – Converts categorical labels into numerical form (**A → 0, B → 1, C → 2**).  \n",
        "\n",
        "🔹 **One-Hot Encoding** – Converts categories into binary columns (useful for non-ordinal categories).  \n",
        "\n",
        "- **Example:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample categorical data\n",
        "\n",
        "data = {'Color': ['Red', 'Blue', 'Green', 'Blue']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Label Encoding\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Color_Label'] = le.fit_transform(df['Color'])\n",
        "\n",
        "# One-Hot Encoding\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "encoded = ohe.fit_transform(df[['Color']])\n",
        "\n",
        "print(\"Label Encoded:\\n\", df)\n",
        "print(\"One-Hot Encoded:\\n\", encoded)\n",
        "```\n",
        "\n",
        "### **3. Binarization (Thresholding Data into 0s and 1s)**  \n",
        "\n",
        "🔹 Converts numerical data into binary values (0 or 1) based on a threshold.  \n",
        "\n",
        "- **Example:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "data = np.array([[1.5], [3.0], [7.8], [2.3]])\n",
        "binarizer = Binarizer(threshold=3.0)\n",
        "binary_data = binarizer.fit_transform(data)\n",
        "\n",
        "print(\"Binarized Data:\\n\", binary_data)\n",
        "```\n",
        "\n",
        "### **4. Polynomial Features (Feature Engineering):**  \n",
        "\n",
        "🔹 **Generates polynomial terms** (e.g., \\( x^2, x^3 \\)) to capture **non-linear relationships** in data.  \n",
        "\n",
        "- **Example:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "data = np.array([[2], [3], [4]])\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "poly_data = poly.fit_transform(data)\n",
        "\n",
        "print(\"Polynomial Features:\\n\", poly_data)\n",
        "```\n",
        "- ### **Summary Table:**  \n",
        "\n",
        "| Function | Purpose | Example Usage |\n",
        "|----------|---------|---------------|\n",
        "| **StandardScaler** | Standardizes to mean 0, variance 1 | `StandardScaler().fit_transform(X)` |\n",
        "| **MinMaxScaler** | Scales data to a fixed range (0 to 1) | `MinMaxScaler().fit_transform(X)` |\n",
        "| **RobustScaler** | Handles outliers (uses median & IQR) | `RobustScaler().fit_transform(X)` |\n",
        "| **LabelEncoder** | Converts categories into numbers | `LabelEncoder().fit_transform(y)` |\n",
        "| **OneHotEncoder** | Converts categories into binary format | `OneHotEncoder().fit_transform(X)` |\n",
        "| **Binarizer** | Converts data into 0 and 1 based on a threshold | `Binarizer(threshold=0.5).fit_transform(X)` |\n",
        "| **PolynomialFeatures** | Creates polynomial terms for feature expansion | `PolynomialFeatures(2).fit_transform(X)` |\n",
        "\n",
        "- ### **Conclusion:**  \n",
        "\n",
        " - The `sklearn.preprocessing` module is **essential** in machine learning for preparing raw data. Without proper preprocessing, models can be biased or inaccurate. **Feature scaling, encoding, binarization, and polynomial feature engineering** improve model performance and interpretability.  \n",
        "\n",
        "---\n",
        "\n",
        "## **24. How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "- When training a machine learning model, we need to evaluate how well it generalizes to unseen data. To do this, we split the dataset into:\n",
        "\n",
        " - **Training Set**: Used to train the model.\n",
        "\n",
        " - **Testing Set**: Used to evaluate model performance.\n",
        "\n",
        "- This prevents **overfitting**, where the model memorizes training data instead of learning patterns.\n",
        "\n",
        "## **1. Using `train_test_split` from Scikit-Learn:**\n",
        "\n",
        "- Scikit-learn provides the `train_test_split()` function, which makes it easy to split data.\n",
        "\n",
        "### **Syntax:**\n",
        "\n",
        "```python\n",
        "train_test_split(X, y, test_size, train_size, random_state, shuffle, stratify)\n",
        "```\n",
        "\n",
        "### **Parameters:**\n",
        "\n",
        "- `X`: Features (independent variables)\n",
        "- `y`: Target (dependent variable)\n",
        "- `test_size`: Proportion of the data for testing (e.g., `0.2` for 20% testing)\n",
        "- `train_size`: Proportion of data for training (optional)\n",
        "- `random_state`: Ensures reproducibility\n",
        "- `shuffle`: Whether to shuffle before splitting (`True` by default)\n",
        "- `stratify`: Ensures balanced class distribution in training and test sets\n",
        "\n",
        "## **2. Example of Data Splitting in Python:**\n",
        "\n",
        "- Let's generate a dataset and split it into training and testing sets.\n",
        "\n",
        "### **Example Code:**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (features X and target y)\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
        "\n",
        "# Splitting the dataset (80% training, 20% testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Training Labels:\\n\", y_train)\n",
        "print(\"Testing Features:\\n\", X_test)\n",
        "print(\"Testing Labels:\\n\", y_test)\n",
        "```\n",
        "\n",
        "## **3. Train-Validation-Test Split:**\n",
        "\n",
        "- In some cases, we also use a **validation set** to fine-tune hyperparameters.\n",
        "\n",
        "### **Steps for Splitting into Train, Validation, and Test Sets:**\n",
        "\n",
        "1. First, split the data into **training + testing**.\n",
        "\n",
        "2. Then, further split the training set into **training + validation**.\n",
        "\n",
        "### **Example Code:**\n",
        "\n",
        "```python\n",
        "# First, split into train + test (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Next, split train into train + validation (80% of train used, 20% for validation)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Final Sizes:\")\n",
        "print(\"Training Set:\", len(X_train))\n",
        "print(\"Validation Set:\", len(X_val))\n",
        "print(\"Testing Set:\", len(X_test))\n",
        "```\n",
        "\n",
        "- Since we first kept **80% for training**, taking **0.25 of that** results in a validation set of **20% of the total data**, maintaining the 60-20-20 split.\n",
        "\n",
        "## **4. Stratified Splitting for Imbalanced Datasets:**\n",
        "\n",
        "- For **classification problems with imbalanced classes**, it's better to ensure class distributions are similar in both train and test sets.\n",
        "\n",
        "### **Example of Stratified Split:**\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "```\n",
        "🔹 The `stratify=y` ensures **equal class proportions** in train and test sets.\n",
        "\n",
        "## **Summary Table:**\n",
        "\n",
        "| **Split Type**       | **Use Case** | **Example Code** |\n",
        "|----------------------|-------------|------------------|\n",
        "| **Train-Test (80-20)** | Basic model evaluation | `train_test_split(X, y, test_size=0.2)` |\n",
        "| **Train-Validation-Test (60-20-20)** | Hyperparameter tuning | Split twice using `train_test_split()` |\n",
        "| **Stratified Split** | Handling imbalanced classes | `train_test_split(X, y, stratify=y)` |\n",
        "\n",
        "## **Conclusion:**\n",
        "\n",
        "- Splitting data properly ensures:\n",
        "\n",
        "✅ **Fair model evaluation**  \n",
        "\n",
        "✅ **Prevention of overfitting**  \n",
        "\n",
        "✅ **Better generalization to new data**  \n",
        "\n",
        "---\n",
        "\n",
        "## **25. Explain data encoding?**  \n",
        "\n",
        "- **Data encoding** is the process of converting **categorical data** (text or labels) into numerical format so that machine learning models can process it effectively. Since most ML algorithms work with numbers, categorical variables need to be transformed before model training.  \n",
        "\n",
        "### **Types of Data Encoding Techniques:-**  \n",
        "\n",
        "### **1. Label Encoding:**  \n",
        "\n",
        "- It assigns a unique numerical value to each category.  \n",
        "\n",
        "**Example:**  \n",
        "\n",
        "| City   | Encoded Value |\n",
        "|--------|--------------|\n",
        "| Delhi  | 0            |\n",
        "| Mumbai | 1            |\n",
        "| Kolkata| 2            |\n",
        "\n",
        "🔹 **Use case:** When categories have an **inherent order** (e.g., Small < Medium < Large).  \n",
        "\n",
        "🔹 **Problem:** Can introduce **ordinal relationships** where they don’t exist.  \n",
        "\n",
        "**Implementation in Python:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Delhi', 'Mumbai', 'Kolkata']\n",
        "encoder = LabelEncoder()\n",
        "encoded_values = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_values)  # Output: [0 1 2]\n",
        "```\n",
        "\n",
        "### **2. One-Hot Encoding (OHE):**  \n",
        "\n",
        "- Converts each category into separate binary columns (0s and 1s).  \n",
        "\n",
        "**Example:**  \n",
        "| City   | Delhi | Mumbai | Kolkata |\n",
        "|--------|-------|--------|---------|\n",
        "| Delhi  | 1     | 0      | 0       |\n",
        "| Mumbai | 0     | 1      | 0       |\n",
        "| Kolkata| 0     | 0      | 1       |\n",
        "\n",
        "🔹 **Use case:** When categories are **nominal (no order)** (e.g., colors: Red, Blue, Green).  \n",
        "\n",
        "🔹 **Problem:** Can create **too many columns** for high-cardinality data.  \n",
        "\n",
        "- **Implementation in Python:**  \n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'City': ['Delhi', 'Mumbai', 'Kolkata']})\n",
        "encoded_df = pd.get_dummies(data, columns=['City'])\n",
        "\n",
        "print(encoded_df)\n",
        "```\n",
        "\n",
        "### **3. Ordinal Encoding:**  \n",
        "\n",
        "- Assigns numbers based on a specific **order or ranking**.  \n",
        "\n",
        "**Example:**  \n",
        "\n",
        "| Size   | Encoded Value |\n",
        "|--------|--------------|\n",
        "| Small  | 1            |\n",
        "| Medium | 2            |\n",
        "| Large  | 3            |\n",
        "\n",
        "🔹 **Use case:** When the categorical values have a **meaningful order** (e.g., experience levels: Beginner < Intermediate < Expert).  \n",
        "\n",
        "- **Implementation in Python:**  \n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Small'], ['Medium'], ['Large']]\n",
        "encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n",
        "encoded_values = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_values)  # Output: [[0] [1] [2]]\n",
        "```\n",
        "\n",
        "### **4. Target Encoding (Mean Encoding):**  \n",
        "\n",
        "- Replaces categories with the **mean of the target variable**.  \n",
        "\n",
        "**Example:** Predicting loan approval (`1` = Approved, `0` = Rejected)  \n",
        "\n",
        "| City   | Approval Rate (Mean) |\n",
        "|--------|----------------------|\n",
        "| Delhi  | 0.7                  |\n",
        "| Mumbai | 0.5                  |\n",
        "| Kolkata| 0.3                  |\n",
        "\n",
        "🔹 **Use case:** For categorical variables in **classification problems**.  \n",
        "\n",
        "🔹 **Problem:** Can lead to **data leakage** if not handled properly.  \n",
        "\n",
        "- **Implementation in Python:**  \n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'City': ['Delhi', 'Mumbai', 'Kolkata', 'Delhi', 'Mumbai'],\n",
        "                   'Loan_Approved': [1, 0, 1, 1, 0]})\n",
        "\n",
        "mean_encoding = df.groupby('City')['Loan_Approved'].mean()\n",
        "df['City_Encoded'] = df['City'].map(mean_encoding)\n",
        "\n",
        "print(df)\n",
        "```\n",
        "\n",
        "### **5. Frequency Encoding:**  \n",
        "\n",
        "- Replaces categories with **the number of times they appear** in the dataset.  \n",
        "\n",
        "**Example:**  \n",
        "\n",
        "| City   | Count |\n",
        "|--------|-------|\n",
        "| Delhi  | 2     |\n",
        "| Mumbai | 2     |\n",
        "| Kolkata| 1     |\n",
        "\n",
        "🔹 **Use case:** When categories have **a large number of unique values**.  \n",
        "\n",
        "🔹 **Problem:** Can fail if counts are **similar across categories**.  \n",
        "\n",
        "- **Implementation in Python:**  \n",
        "\n",
        "```python\n",
        "df['City_Frequency'] = df['City'].map(df['City'].value_counts())\n",
        "```\n",
        "\n",
        "### **6. Binary Encoding:**  \n",
        "\n",
        "- Converts categories into binary form and splits them into separate columns.  \n",
        "\n",
        "**Example:**  \n",
        "\n",
        "| City   | Binary  | Col1 | Col2 |\n",
        "|--------|--------|------|------|\n",
        "| Delhi  | 00     | 0    | 0    |\n",
        "| Mumbai | 01     | 0    | 1    |\n",
        "| Kolkata| 10     | 1    | 0    |\n",
        "\n",
        "🔹 **Use case:** When one-hot encoding results in **too many columns**.  \n",
        "\n",
        "- **Implementation in Python:**  \n",
        "\n",
        "```python\n",
        "from category_encoders import BinaryEncoder\n",
        "\n",
        "encoder = BinaryEncoder(cols=['City'])\n",
        "df_encoded = encoder.fit_transform(df)\n",
        "print(df_encoded)\n",
        "```\n",
        "\n",
        "| Data Type      | Best Encoding Techniques |\n",
        "|---------------|----------------------|\n",
        "| **Ordinal (Ordered)** | Ordinal Encoding, Label Encoding |\n",
        "| **Nominal (Unordered, Few Categories)** | One-Hot Encoding, Binary Encoding |\n",
        "| **Nominal (Unordered, Many Categories)** | Target Encoding, Frequency Encoding |\n",
        "\n",
        "### **Conclusion:**  \n",
        "\n",
        "✅ Encoding is crucial to prepare categorical data for machine learning.  \n",
        "\n",
        "✅ Choose the **right encoding method** based on data type and problem type.  \n",
        "\n",
        "✅ Be mindful of **data leakage** when using Target Encoding.  \n",
        "\n"
      ],
      "metadata": {
        "id": "6bpF5QRNK1Gd"
      }
    }
  ]
}